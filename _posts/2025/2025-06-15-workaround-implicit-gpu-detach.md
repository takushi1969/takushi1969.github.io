---
layout: post
title:  Workaround for occasional implicit GPU detachment in Docker
date:   2025-06-15
tags:   ubuntu docker ollama llm gpu nvidia
---
## èª²é¡Œ

[Ubuntu
24.04 LTS](https://canonical.com/blog/canonical-releases-ubuntu-24-04-noble-numbat)
ã®ãƒ‡ã‚£ã‚¹ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹docker.ioã«å¯¾ã—ã¦ã€[Nvidia docker
container](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/index.html)
ã§Nvidia GPUã‚’è²¼ã‚Šä»˜ã‘ãŸDockerã‚³ãƒ³ãƒ†ãƒŠã‚’é•·æœŸãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã—ã¦ã„ã‚‹ã¨ã€ã„ã¤
ã®é–“ã«ã‹GPUãŒå¤–ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚

ã“ã®äº‹è±¡ãŒç™ºç”Ÿã—ã¦ã„ã‚‹ã¨ã€Dockerã‚³ãƒ³ãƒ†ãƒŠã®ã‚µãƒ¼ãƒ“ã‚¹ãŒä½¿ç”¨ã§ããªã‹ã£ãŸã‚Šã€
Dockerã‚³ãƒ³ãƒ†ãƒŠã®ã‚µãƒ¼ãƒ“ã‚¹ä½¿ç”¨æ™‚ã®GPUã®ä»£ã‚ã‚Šã«CPUè² è·ãŒæ€¥ä¸Šæ˜‡ã—ãŸã‚Šã™ã‚‹ã€‚
ãã—ã¦ã€Dockerã®ãƒ­ã‚°ã«ã¯ã€GPUãŒåˆ©ç”¨ã§ããªã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå‡ºç¾ã—ã¦ã„ãŸã‚Š
ã™ã‚‹ã€‚

ä¾‹ãˆã°ã€[ollama/ollama](https://hub.docker.com/r/ollama/ollama)ã®å ´åˆã€
ã“ã®ç¾è±¡ãŒç™ºç¾ã™ã‚‹ã¨ã€ä»¥ä¸‹ã«ç¤ºã™ã‚ˆã†ã«`no CUDA-capable device`ã‚’å«ã‚“ã 
ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå‡ºåŠ›ã•ã‚Œã‚‹ã€‚
![no CUDA-capable device](/assets/images/2025/0614/no-CUDA-capeble.png)

## å›é¿æ–¹æ³•

ã“ã®å•é¡Œã¯[NOTICE: Containers losing access to GPUs with error:
"Failed to initialize NVML: Unknown Error"
#48](https://github.com/NVIDIA/nvidia-container-toolkit/issues/48)ç­‰ã§åºƒã
çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ã€‚ãã—ã¦ã€ãã®äº‹è±¡ã¯ã€`systemctl daemon-reload`ã®å®Ÿè¡Œã«ã‚ˆã£
ã¦ç°¡å˜ã«èª˜ç™ºã•ã‚Œã‚‹ã“ã¨ãŒè¨˜ã•ã‚Œã¦ã„ã‚‹ã€‚ã¾ãŸã€åŒè¨˜äº‹ã«ã¯ã€Œ5. Workaroundsã€
ã§3ã¤ã®å›é¿æ–¹æ³•ãŒææ¡ˆã•ã‚Œã¦ã„ã‚‹ã€‚

1. nvidia-ctkã®ä½¿ç”¨
2. æ˜ç¤ºçš„ãªsystemd cgroupç®¡ç†ã®ç„¡åŠ¹åŒ–
3. dockerã®ãƒ€ã‚¦ãƒ³ã‚°ãƒ¬ãƒ¼ãƒ‰

ä»Šå›ã¯ã€ã“ã®3ã¤ã®æ–¹æ³•ã®ä¸­ã€ä¸€ç•ªå®¹æ˜“ã¨æ€ã‚ã‚Œã‚‹2ã§äº‹æ…‹ãŒè§£æ¶ˆã•ã‚Œã‚‹ã“ã¨ã‚’
ç¢ºèªã—ãŸã€‚

ä»¥ä¸‹ã«äº‹è±¡ã®ç™ºç”Ÿã®ç¢ºèªã¨å›é¿æ–¹æ³•ã®æ¤œè¨¼å‹•ç”»ã‚’ç¤ºã™ã€‚æ¤œè¨¼ã«ã¯ã€Dockerã‚³ãƒ³
ãƒ†ãƒŠå†…ã§ã®`torch.cuda.is_available()`ã«ã‚ˆã‚‹GPUã®ä½¿ç”¨å¯å¦ã§ç¢ºèªã—ãŸã€‚

<video autoplay loop width=640px controls muted>
    <source src="/assets/images/2025/0614/workaround_for_implicit_gpu_detach.webm">
</video>

## æ³¨æ„

ä»Šå›ã®å›é¿ç­–ã®æ¤œè¨¼ã§ã¯ã€Dockerã®Cgroup Driverã‚’systemdã‹ã‚‰cgroupfsã«å¤‰æ›´ã™ã‚‹
ã“ã¨ã§å®Ÿç¾ã—ã¦ã„ã‚‹ã€‚ã¤ã¾ã‚Šã€Dockerå…¨ä½“ã«å½±éŸ¿ã™ã‚‹ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚
ã“ã®ãŸã‚ã€è£½å“ã‚„ã‚µãƒ¼ãƒ“ã‚¹ã¸ã®é©ç”¨ã™ã‚‹å ´åˆã«ã¯ã€ååˆ†ãªè¦‹æ¥µã‚ãŒå¿…è¦ã¨æ€ã‚ã‚Œã‚‹ã€‚

## å‚è€ƒ
ä»Šå›ã®å›é¿ç­–(systemd â†’ cgroupfs)ã«ã¤ã„ã¦ã€ChatGPTã«å½±éŸ¿ã‚’å•åˆã›ãŸçµæœã€
ä»¥ä¸‹ã®å›ç­”ã¨ãªã£ãŸ(å›ç­”å†…å®¹ã®æ­£ç¢ºæ€§ã¯æœªç¢ºèª)ã€‚

> teach me the impact of changing the cgroup driver from `systemd` to `cgroupfs`.
>
> ### ChatGPT:
>
> Changing the cgroup driver in container runtimes like Docker or
> containerd from `systemd` to `cgroupfs` can significantly affect how your
> containers interact with system resources. Here's a breakdown to help
> you understand the impact of this change:
>
> -------------------------------------------------------------------------------
>
> #### ğŸ”§ What is a Cgroup Driver?
>
> Cgroup (control group) drivers are used by container runtimes to
> manage and limit resources (CPU, memory, I/O) for containers using
> Linux cgroups.
>
> - `systemd` driver: Integrates with the `systemd` init system to manage cgroups.
> - `cgroupfs` driver: Uses the legacy cgroup filesystem interface directly, independently of `systemd`.
>
> -------------------------------------------------------------------------------
>
> #### ğŸ§  Conceptual Difference
>
> | Feature          | `systemd` Driver                                     | `cgroupfs` Driver                          |
> |------------------|----------------------------------------------------|------------------------------------------|
> | Integration      | Tight integration with `systemd`                     | Independent of `systemd`                   |
> | Cgroup hierarchy | Unified and consistent                             | May conflict with `systemd`â€™s own cgroups |
> | OS compatibility | Best on `systemd`-based systems (e.g., Ubuntu, RHEL) | More universal but less consistent       |
>
> -------------------------------------------------------------------------------
>
> #### âš ï¸ Impact of Switching to `cgroupfs`
>
> ##### âœ… Pros
>
> - Compatibility with non-systemd systems: Useful on systems without `systemd` (like Alpine Linux).
> - Simplicity: Sometimes easier to debug since it doesnâ€™t interweave with `systemd`â€™s own cgroups.
>
> ##### âŒ Cons
>
> 1. Potential conflicts:
>     - `systemd` and `cgroupfs` may manage the same hierarchy, causing conflicts in ownership or nested cgroups.
>     - You may see errors like:
>
> 		```sql
> 		failed to create shim task: OCI runtime create failed: ... cgroup conflict: ...: unknown
>         ```
> 2. Monitoring and tooling inconsistency:
> - Tools that rely on `systemd` cgroup hierarchy (e.g., `systemd-cgtop`, `journalctl`) may not accurately report container metrics.
> 3. Cluster management issues (e.g., Kubernetes):
>     - Kubernetes expects node cgroup drivers to match between the kubelet and container runtime. Mismatched drivers (e.g., kubelet using `systemd`, but Docker using `cgroupfs`) can cause:
>         - Pod startup failures
> 		- Resource management errors
> 		- Node tainting or eviction
> 4. Security & isolation nuances:
>     - `systemd` may better isolate services by placing each in its own slice and controlling them more granularly.
>
> -------------------------------------------------------------------------------
>
> #### ğŸ§© Summary
> Changing the cgroup driver from `systemd` to `cgroupfs`:
> - Is not just a cosmetic or config change â€” it affects how containers are integrated with your OS.
> - May lead to issues if your system or Kubernetes setup assumes a specific driver.
> - Should be done consistently across your container runtime and orchestration components.
